{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXsjG7395wRG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "from PIL import Image, ImageFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import sys\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "\n",
        "\n",
        "CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
        "D_CLIP_HIDDEN = 1024\n",
        "RESIDUAL_ALPHA = 0.2\n",
        "COSINE_CLASSIFIER_SCALE = 20.0\n",
        "DROPOUT_RATE = 0.5\n",
        "TASK_C = 'label'\n",
        "OUTPUT_SIZE_C = 3\n",
        "LABEL_TO_ID_C = {'Neutral': 0, 'Support': 1, 'Oppose': 2}\n",
        "ID_TO_LABEL_C = {v: k for k, v in LABEL_TO_ID_C.items()}\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "HEAD_ONLY_EPOCHS = 8\n",
        "FULL_MODEL_EPOCHS = 20\n",
        "LAYERS_TO_UNFREEZE = 2\n",
        "\n",
        "\n",
        "MAX_LR_HEAD_STAGE1 = 1e-4\n",
        "MAX_LR_HEAD_STAGE2 = 2e-5\n",
        "MAX_LR_BACKBONE = 1e-8\n",
        "WEIGHT_DECAY = 1e-2\n",
        "GRADIENT_CLIP_VAL = 1.0\n",
        "\n",
        "\n",
        "LABEL_SMOOTHING = 0.15\n",
        "SWA_START_EPOCH = 6\n",
        "SWA_LR = 5e-5\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed);\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Random seed set to {seed} for reproducibility.\")\n",
        "\n",
        "\n",
        "def load_and_combine_data_C(base_dir):\n",
        "    \"\"\"Loads and prepares train/validation dataframes from the specified base directory.\"\"\"\n",
        "\n",
        "    train_images_root_dir = os.path.join(base_dir, \"Subtask_C\", \"Train\", \"Subtask_C_Train\")\n",
        "    train_text_csv_file = os.path.join(base_dir, \"Train_Text\", \"STask_C_train.csv\")\n",
        "    val_images_dir = os.path.join(base_dir, \"Subtask_C\", \"Evaluation\", \"STask_C_val_img\")\n",
        "    val_text_csv_file = os.path.join(base_dir, \"Eval_Data_Text\", \"STask-C(index,text)val.csv\")\n",
        "    val_labels_csv_file = os.path.join(base_dir, \"Eval_Data_Labels\", \"STask-C(index,label)val.csv\")\n",
        "\n",
        "    print(\"\\n--- Loading Training Data (Subtask C) ---\")\n",
        "    train_text_df = pd.read_csv(train_text_csv_file).rename(columns={'index': 'name', 'text': 'text'})\n",
        "    train_data = []\n",
        "    for category, label_id in LABEL_TO_ID_C.items():\n",
        "        folder = os.path.join(train_images_root_dir, category)\n",
        "        if not os.path.isdir(folder):\n",
        "            print(f\"Warning: Training directory not found, skipping: {folder}\")\n",
        "            continue\n",
        "        for img_path in glob.glob(os.path.join(folder, '*.*')):\n",
        "            name = os.path.basename(img_path)\n",
        "            text_val = train_text_df[train_text_df['name'] == name]['text'].values\n",
        "            text = str(text_val[0]) if len(text_val) > 0 and pd.notna(text_val[0]) else \"\"\n",
        "            train_data.append({'name': name, 'text': text, 'label': label_id, 'img_path': img_path, 'split': 'train'})\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "\n",
        "    print(\"\\n--- Loading Validation Data (Subtask C) ---\")\n",
        "    val_labels_df = pd.read_csv(val_labels_csv_file).rename(columns={'index': 'name', 'label': 'label'})\n",
        "    val_text_df = pd.read_csv(val_text_csv_file).rename(columns={'index': 'name', 'text': 'text'})\n",
        "    val_df = pd.merge(val_labels_df, val_text_df, on='name', how='inner')\n",
        "    val_df.dropna(subset=['label'], inplace=True)\n",
        "    val_df['label'] = val_df['label'].astype(int)\n",
        "    val_df['img_path'] = val_df['name'].apply(lambda x: os.path.join(val_images_dir, x))\n",
        "    val_df['split'] = 'val'\n",
        "\n",
        "    print(\"\\n--- Data Loading Summary ---\")\n",
        "    all_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    print(f\"Loaded {len(train_df)} training samples and {len(val_df)} validation samples.\")\n",
        "    if len(train_df) == 0 or len(val_df) == 0:\n",
        "        sys.exit(\"FATAL: Training or validation data is empty. Check paths and files.\")\n",
        "    return all_df\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, data_df, processor, split, transform):\n",
        "        self.data = data_df[data_df['split'] == split].reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        label = torch.tensor(item['label'], dtype=torch.long)\n",
        "        text = self.processor.tokenizer(str(item['text']), return_tensors=\"pt\", padding='max_length', truncation=True, max_length=77).input_ids.squeeze(0)\n",
        "        try: image = self.transform(Image.open(item['img_path']).convert(\"RGB\"))\n",
        "        except Exception: image = torch.zeros((3, 224, 224), dtype=torch.float)\n",
        "        return {\"image\": image, \"text\": text, \"label\": label}\n",
        "\n",
        "\n",
        "class FeatureAdapter(nn.Module):\n",
        "    def __init__(self, dim): super().__init__(); self.adapter = nn.Sequential(nn.Linear(dim, dim//2), nn.GELU(), nn.Linear(dim//2, dim))\n",
        "    def forward(self, x, alpha): return alpha * self.adapter(x) + (1.0 - alpha) * x\n",
        "\n",
        "class CosineClassifier(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, scale, prompts, clip_model_ref, processor_ref):\n",
        "        super().__init__(); self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim)); self.scale = scale\n",
        "        with torch.no_grad():\n",
        "            sai_proj = nn.Linear(clip_model_ref.config.projection_dim, in_dim).to(clip_model_ref.device)\n",
        "            tokenized = processor_ref.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(clip_model_ref.device)\n",
        "            raw_embeds = clip_model_ref.get_text_features(**tokenized)\n",
        "            self.weight.data = F.normalize(sai_proj(raw_embeds.float()).data, p=2, dim=1)\n",
        "    def forward(self, x): return F.linear(F.normalize(x, p=2, dim=1), F.normalize(self.weight, p=2, dim=1)) * self.scale\n",
        "\n",
        "class FinalMemeCLIP(nn.Module):\n",
        "    def __init__(self, clip_model_ref, processor_ref, clip_output_dim):\n",
        "        super().__init__()\n",
        "        self.clip_model = clip_model_ref\n",
        "        self.image_proj = nn.Linear(clip_output_dim, D_CLIP_HIDDEN)\n",
        "        self.text_proj = nn.Linear(clip_output_dim, D_CLIP_HIDDEN)\n",
        "        self.image_adapter = FeatureAdapter(D_CLIP_HIDDEN)\n",
        "        self.text_adapter = FeatureAdapter(D_CLIP_HIDDEN)\n",
        "        self.pre_output_layer = nn.Sequential(nn.Linear(D_CLIP_HIDDEN, D_CLIP_HIDDEN), nn.GELU(), nn.Dropout(DROPOUT_RATE))\n",
        "        prompts = [f\"a meme expressing a '{ID_TO_LABEL_C[i].lower()}' stance\" for i in sorted(ID_TO_LABEL_C.keys())]\n",
        "        self.classifier = CosineClassifier(D_CLIP_HIDDEN, OUTPUT_SIZE_C, COSINE_CLASSIFIER_SCALE, prompts, self.clip_model, processor_ref)\n",
        "    def forward(self, image, text):\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.get_image_features(image)\n",
        "            text_features = self.clip_model.get_text_features(text)\n",
        "        img_proj, txt_proj = self.image_proj(image_features.float()), self.text_proj(text_features.float())\n",
        "        img_adapted, txt_adapted = self.image_adapter(img_proj, RESIDUAL_ALPHA), self.text_adapter(txt_proj, RESIDUAL_ALPHA)\n",
        "        fused = self.pre_output_layer(img_adapted * txt_adapted)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "def set_requires_grad(model, requires_grad):\n",
        "    for param in model.parameters(): param.requires_grad = requires_grad\n",
        "\n",
        "def unfreeze_last_n_layers(clip_model, n):\n",
        "    for layer in list(clip_model.vision_model.encoder.layers)[-n:]: set_requires_grad(layer, True)\n",
        "    for layer in list(clip_model.text_model.encoder.layers)[-n:]: set_requires_grad(layer, True)\n",
        "    print(f\"Unfroze last {n} layers of CLIP vision and text encoders for fine-tuning.\")\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, criterion, optimizer, scheduler, scaler):\n",
        "    model.train(); total_loss = 0; all_preds, all_labels = [], []\n",
        "    pbar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
        "    for data in pbar:\n",
        "        img, txt, label = data['image'].to(DEVICE), data['text'].to(DEVICE), data['label'].to(DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(img, txt); loss = criterion(logits, label)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VAL)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if scheduler: scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "        all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy()); all_labels.extend(label.cpu().numpy())\n",
        "    return total_loss/len(data_loader), f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "def validate_epoch(model, data_loader, criterion):\n",
        "    model.eval(); total_loss = 0; all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
        "            img, txt, label = data['image'].to(DEVICE), data['text'].to(DEVICE), data['label'].to(DEVICE)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(img, txt); loss = criterion(logits, label)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy()); all_labels.extend(label.cpu().numpy())\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    return total_loss/len(data_loader), f1, prec, rec, acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train MemeCLIP model for Subtask C: Stance Detection.\")\n",
        "    parser.add_argument(\n",
        "        '--base_dir',\n",
        "        type=str,\n",
        "        default=\"./SharedTaskProject\",\n",
        "        help=\"Path to the base project directory containing the Subtask data folders.\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    seed_everything(SEED)\n",
        "\n",
        "\n",
        "    BASE_PROJECT_DIR = args.base_dir\n",
        "    MODELS_DIR_C = os.path.join(BASE_PROJECT_DIR, \"models\", \"Subtask_C\")\n",
        "    EXP_NAME_C = \"MemeCLIP_Stance_F1_Breakthrough_vFinal\"\n",
        "    MODEL_SAVE_PATH_C = os.path.join(MODELS_DIR_C, EXP_NAME_C)\n",
        "    Path(MODEL_SAVE_PATH_C).mkdir(parents=True, exist_ok=True)\n",
        "    BEST_MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_PATH_C, \"best_model_subtask_C.pt\")\n",
        "    SWA_MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_PATH_C, \"swa_model_subtask_C.pt\")\n",
        "    print(f\"This run's models will be saved in: {MODEL_SAVE_PATH_C}\")\n",
        "\n",
        "\n",
        "    print(f\"Loading CLIP: {CLIP_MODEL_ID}...\")\n",
        "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
        "    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE)\n",
        "    CLIP_OUTPUT_DIM = clip_model.config.projection_dim\n",
        "\n",
        "    all_data_df = load_and_combine_data_C(BASE_PROJECT_DIR)\n",
        "\n",
        "\n",
        "    norm_mean, norm_std = processor.image_processor.image_mean, processor.image_processor.image_std\n",
        "    val_transform = transforms.Compose([transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
        "    train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=(0.75, 1.0), interpolation=transforms.InterpolationMode.BICUBIC), transforms.RandomHorizontalFlip(), transforms.TrivialAugmentWide(interpolation=transforms.InterpolationMode.BICUBIC), transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n",
        "\n",
        "    train_dataset = MultimodalDataset(all_data_df, processor, 'train', train_transform)\n",
        "    val_dataset = MultimodalDataset(all_data_df, processor, 'val', val_transform)\n",
        "    dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    train_labels = all_data_df[all_data_df['split']=='train']['label']\n",
        "    class_weights = torch.tensor(compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels.to_numpy()), dtype=torch.float).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
        "    print(f\"Using CrossEntropyLoss with Label Smoothing: {LABEL_SMOOTHING} and Weights: {class_weights.cpu().numpy()}\")\n",
        "\n",
        "    full_model = FinalMemeCLIP(clip_model, processor, CLIP_OUTPUT_DIM).to(DEVICE)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_f1 = -1.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"\\n--- STAGE 1: WARMING UP CLASSIFIER HEAD FOR {HEAD_ONLY_EPOCHS} EPOCHS ---\")\n",
        "    set_requires_grad(full_model.clip_model, False)\n",
        "    set_requires_grad(full_model.classifier, True)\n",
        "    set_requires_grad(full_model.pre_output_layer, True)\n",
        "    set_requires_grad(full_model.image_adapter, True)\n",
        "    set_requires_grad(full_model.text_adapter, True)\n",
        "    set_requires_grad(full_model.image_proj, True)\n",
        "    set_requires_grad(full_model.text_proj, True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, full_model.parameters()), lr=MAX_LR_HEAD_STAGE1, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=MAX_LR_HEAD_STAGE1, epochs=HEAD_ONLY_EPOCHS, steps_per_epoch=len(dataloader_train))\n",
        "\n",
        "    for epoch in range(HEAD_ONLY_EPOCHS):\n",
        "        train_loss, train_f1 = train_epoch(full_model, dataloader_train, criterion, optimizer, scheduler, scaler)\n",
        "        val_loss, val_f1, val_p, val_r, val_acc = validate_epoch(full_model, dataloader_val, criterion)\n",
        "        print(f\"[Head Only] Epoch {epoch+1}/{HEAD_ONLY_EPOCHS} -> Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Val P: {val_p:.4f} | Val R: {val_r:.4f}\")\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(full_model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"** New best model saved in Stage 1! Val F1: {best_val_f1:.4f} **\")\n",
        "\n",
        "\n",
        "    print(f\"\\n--- STAGE 2: ULTRA-GENTLE FINE-TUNING & SWA ---\")\n",
        "    full_model.load_state_dict(torch.load(BEST_MODEL_SAVE_PATH))\n",
        "    unfreeze_last_n_layers(full_model.clip_model, LAYERS_TO_UNFREEZE)\n",
        "\n",
        "    param_groups = [{'params': filter(lambda p: p.requires_grad, full_model.clip_model.parameters()), 'lr': MAX_LR_BACKBONE},\n",
        "                    {'params': [p for name, p in full_model.named_parameters() if 'clip_model' not in name and p.requires_grad], 'lr': MAX_LR_HEAD_STAGE2}]\n",
        "    optimizer = torch.optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=[MAX_LR_BACKBONE, MAX_LR_HEAD_STAGE2], total_steps=FULL_MODEL_EPOCHS * len(dataloader_train))\n",
        "\n",
        "    swa_model = AveragedModel(full_model)\n",
        "    swa_scheduler = SWALR(optimizer, swa_lr=SWA_LR)\n",
        "    print(f\"SWA initialized. Will start averaging after epoch {SWA_START_EPOCH} of full-tuning.\")\n",
        "\n",
        "    for epoch in range(FULL_MODEL_EPOCHS):\n",
        "        current_epoch_num = HEAD_ONLY_EPOCHS + epoch + 1\n",
        "        train_loss, train_f1 = train_epoch(full_model, dataloader_train, criterion, optimizer, scheduler, scaler)\n",
        "\n",
        "        val_loss, val_f1, val_p, val_r, val_acc = validate_epoch(full_model, dataloader_val, criterion)\n",
        "        print(f\"[Full Tune] Epoch {current_epoch_num} -> Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Val P: {val_p:.4f} | Val R: {val_r:.4f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(full_model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"** New best standard model saved! Val F1: {best_val_f1:.4f} **\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"Validation F1 did not improve. Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "        if epoch >= SWA_START_EPOCH:\n",
        "            swa_model.update_parameters(full_model)\n",
        "            swa_scheduler.step()\n",
        "            print(\"SWA model updated.\")\n",
        "\n",
        "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered after {EARLY_STOPPING_PATIENCE} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "\n",
        "    print(\"\\n--- FINAL EVALUATION USING SWA MODEL FOR MAXIMUM GENERALIZATION ---\")\n",
        "    torch.optim.swa_utils.update_bn(dataloader_train, swa_model, device=DEVICE)\n",
        "    swa_val_loss, swa_val_f1, swa_val_p, swa_val_r, swa_val_acc = validate_epoch(swa_model, dataloader_val, criterion)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n               TRAINING COMPLETE\\n\" + \"=\"*50)\n",
        "    print(f\"Best Standard Model (val F1): {best_val_f1:.5f}\\nFinal SWA Model (val F1):     {swa_val_f1:.5f}\")\n",
        "    print(f\"SWA Metrics -> P: {swa_val_p:.4f}, R: {swa_val_r:.4f}, Acc: {swa_val_acc:.4f}\")\n",
        "\n",
        "    torch.save(swa_model.state_dict(), SWA_MODEL_SAVE_PATH)\n",
        "    print(f\" SWA model saved to {SWA_MODEL_SAVE_PATH}\")"
      ]
    }
  ]
}