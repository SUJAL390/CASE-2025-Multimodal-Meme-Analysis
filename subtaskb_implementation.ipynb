{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qqg7-qx8_0Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import sys\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
        "OUTPUT_SIZE_B = 4\n",
        "LABEL_TO_ID_B = {'Undirected': 0, 'Individual': 1, 'Community': 2, 'Organization': 3}\n",
        "\n",
        "SEED = 42\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUMULATION_STEPS = 8\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 15\n",
        "MAX_LR = 2e-5\n",
        "WEIGHT_DECAY = 1e-1\n",
        "LABEL_SMOOTHING = 0.1\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed);\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n",
        "    print(f\"Random seed set to {seed} for reproducibility.\")\n",
        "\n",
        "\n",
        "def load_and_combine_data_B(base_dir):\n",
        "    \"\"\"Loads and prepares train/validation dataframes from the specified base directory.\"\"\"\n",
        "\n",
        "    train_images_root_dir = os.path.join(base_dir, \"Subtask_B\", \"Train\", \"Subtask_B_Train\")\n",
        "    train_text_csv_file = os.path.join(base_dir, \"Train_Text\", \"STask_B_train.csv\")\n",
        "    val_images_dir = os.path.join(base_dir, \"Subtask_B\", \"Evaluation\", \"STask_B_val_img\")\n",
        "    val_text_csv_file = os.path.join(base_dir, \"Eval_Data_Text\", \"STask-B(index,text)val.csv\")\n",
        "    val_labels_csv_file = os.path.join(base_dir, \"Eval_Data_Labels\", \"STask-B(index,label)val.csv\")\n",
        "\n",
        "    train_text_df = pd.read_csv(train_text_csv_file)\n",
        "    if 'name' not in train_text_df.columns and 'index' in train_text_df.columns:\n",
        "        train_text_df.rename(columns={'index': 'name'}, inplace=True)\n",
        "    train_text_df.set_index('name', inplace=True)\n",
        "\n",
        "    train_data = []\n",
        "    for category, label_id in LABEL_TO_ID_B.items():\n",
        "        folder_path = os.path.join(train_images_root_dir, category)\n",
        "        image_files = glob.glob(os.path.join(folder_path, '*.*'))\n",
        "        for img_path in image_files:\n",
        "            img_filename = os.path.basename(img_path)\n",
        "            try: text = str(train_text_df.loc[img_filename]['text'])\n",
        "            except KeyError: text = \"\"\n",
        "            train_data.append({'name': img_filename, 'text': text, 'label': label_id, 'img_path': img_path, 'split': 'train'})\n",
        "    train_df = pd.DataFrame(train_data)\n",
        "\n",
        "    val_labels_df = pd.read_csv(val_labels_csv_file)\n",
        "    val_text_df = pd.read_csv(val_text_csv_file)\n",
        "    if 'name' not in val_labels_df.columns and 'index' in val_labels_df.columns: val_labels_df.rename(columns={'index': 'name'}, inplace=True)\n",
        "    if 'name' not in val_text_df.columns and 'index' in val_text_df.columns: val_text_df.rename(columns={'index': 'name'}, inplace=True)\n",
        "    val_df = pd.merge(val_labels_df, val_text_df, on='name', how='inner')\n",
        "    val_df.dropna(subset=['label'], inplace=True); val_df['label'] = val_df['label'].astype(int)\n",
        "    val_df['img_path'] = val_df['name'].apply(lambda x: os.path.join(val_images_dir, x)); val_df['split'] = 'val'\n",
        "    return pd.concat([train_df, val_df], ignore_index=True)\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, transform):\n",
        "        self.df, self.tokenizer, self.transform = df, tokenizer, transform\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = self.tokenizer(str(row['text']), padding='max_length', max_length=77, truncation=True, return_tensors=\"pt\")\n",
        "        label = torch.tensor(row['label'], dtype=torch.long)\n",
        "        try: image = self.transform(Image.open(row['img_path']).convert(\"RGB\"))\n",
        "        except Exception: image = torch.zeros((3, 224, 224), dtype=torch.float32)\n",
        "        return {'image': image, 'text_input_ids': text['input_ids'].squeeze(), 'text_attention_mask': text['attention_mask'].squeeze(), 'label': label}\n",
        "\n",
        "\n",
        "class CoFTModel(nn.Module):\n",
        "    def __init__(self, clip_model, num_classes, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.clip = clip_model\n",
        "        embed_dim = self.clip.config.projection_dim\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8, dim_feedforward=embed_dim*4, dropout=dropout, activation='gelu', batch_first=True)\n",
        "        self.text_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.image_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.cross_attention = nn.MultiheadAttention(embed_dim, num_heads=8, dropout=dropout, batch_first=True)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim * 2),\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, text_input_ids, text_attention_mask):\n",
        "        image_feat = self.clip.get_image_features(pixel_values=image).unsqueeze(1)\n",
        "        text_feat = self.clip.get_text_features(input_ids=text_input_ids, attention_mask=text_attention_mask).unsqueeze(1)\n",
        "        refined_image_feat = self.image_encoder(image_feat)\n",
        "        refined_text_feat = self.text_encoder(text_feat)\n",
        "        text_q_img_context, _ = self.cross_attention(query=refined_text_feat, key=refined_image_feat, value=refined_image_feat)\n",
        "        fused_feat = torch.cat([refined_text_feat, text_q_img_context], dim=-1).squeeze(1)\n",
        "        logits = self.head(fused_feat)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_epoch(model, data_loader, criterion, optimizer, scheduler, scaler):\n",
        "    model.train(); total_loss = 0; all_preds, all_labels = [], []\n",
        "    optimizer.zero_grad()\n",
        "    for i, batch in enumerate(tqdm(data_loader, desc=\"Training\", leave=False)):\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        input_ids = batch['text_input_ids'].to(DEVICE)\n",
        "        attention_mask = batch['text_attention_mask'].to(DEVICE)\n",
        "        labels = batch['label'].to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(images, input_ids, attention_mask)\n",
        "            loss = criterion(logits, labels) / GRAD_ACCUMULATION_STEPS\n",
        "        scaler.scale(loss).backward()\n",
        "        if (i + 1) % GRAD_ACCUMULATION_STEPS == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "        total_loss += loss.item() * GRAD_ACCUMULATION_STEPS\n",
        "        all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
        "    return total_loss/len(data_loader), f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "def validate_epoch(model, data_loader, criterion):\n",
        "    model.eval(); total_loss = 0; all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            input_ids = batch['text_input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['text_attention_mask'].to(DEVICE)\n",
        "            labels = batch['label'].to(DEVICE)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(images, input_ids, attention_mask)\n",
        "                loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            all_preds.extend(torch.argmax(logits, dim=1).cpu().numpy()); all_labels.extend(labels.cpu().numpy())\n",
        "    return total_loss/len(data_loader), f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train CoFT model for Subtask B: Target Classification.\")\n",
        "    parser.add_argument(\n",
        "        '--base_dir',\n",
        "        type=str,\n",
        "        default=\"./SharedTaskProject\",\n",
        "        help=\"Path to the base project directory containing the Subtask data folders.\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    seed_everything(SEED)\n",
        "\n",
        "\n",
        "    BASE_PROJECT_DIR = args.base_dir\n",
        "    CURRENT_EXP_NAME = \"CoFT_TargetClassification_EndToEnd_v1\"\n",
        "    MODEL_SAVE_PATH_B = os.path.join(BASE_PROJECT_DIR, \"models\", \"Subtask_B\", CURRENT_EXP_NAME)\n",
        "    Path(MODEL_SAVE_PATH_B).mkdir(parents=True, exist_ok=True)\n",
        "    BEST_MODEL_SAVE_PATH = os.path.join(MODEL_SAVE_PATH_B, \"best_model_subtask_B.pt\")\n",
        "    print(f\"This run's models will be saved in: {MODEL_SAVE_PATH_B}\")\n",
        "\n",
        "    print(\"Loading CLIP base model and processor...\")\n",
        "    clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID)\n",
        "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
        "\n",
        "    print(\"Loading and preparing dataframes...\")\n",
        "    all_data_df = load_and_combine_data_B(BASE_PROJECT_DIR)\n",
        "    train_df = all_data_df[all_data_df['split'] == 'train'].reset_index(drop=True)\n",
        "    val_df = all_data_df[all_data_df['split'] == 'val'].reset_index(drop=True)\n",
        "\n",
        "\n",
        "    norm_mean, norm_std = processor.image_processor.image_mean, processor.image_processor.image_std\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandAugment(num_ops=2, magnitude=10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=norm_mean, std=norm_std)])\n",
        "\n",
        "    train_dataset = MultimodalDataset(train_df, processor.tokenizer, train_transform)\n",
        "    val_dataset = MultimodalDataset(val_df, processor.tokenizer, val_transform)\n",
        "    dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    dataloader_val = DataLoader(val_dataset, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Training Co-Attentional Fusion Transformer (CoFT) End-to-End ---\")\n",
        "    model = CoFTModel(clip_model, OUTPUT_SIZE_B).to(DEVICE)\n",
        "\n",
        "    class_weights = torch.tensor(compute_class_weight('balanced', classes=np.unique(train_df['label']), y=train_df['label']), dtype=torch.float).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTHING)\n",
        "\n",
        "    head_params = [p for n, p in model.named_parameters() if 'clip' not in n]\n",
        "    backbone_params = [p for n, p in model.named_parameters() if 'clip' in n]\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': head_params, 'lr': MAX_LR},\n",
        "        {'params': backbone_params, 'lr': MAX_LR / 10}\n",
        "    ], weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=[MAX_LR, MAX_LR/10],\n",
        "                                                    epochs=NUM_EPOCHS,\n",
        "                                                    steps_per_epoch=len(dataloader_train)//GRAD_ACCUMULATION_STEPS,\n",
        "                                                    pct_start=0.2)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    best_val_f1 = -1.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_loss, train_f1 = train_epoch(model, dataloader_train, criterion, optimizer, scheduler, scaler)\n",
        "        val_loss, val_f1 = validate_epoch(model, dataloader_val, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Head LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), BEST_MODEL_SAVE_PATH)\n",
        "            print(f\"** New best model saved! Val F1: {val_f1:.4f} **\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"Validation F1 did not improve. Patience: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n TRAINING COMPLETE\\n\" + \"=\"*50)\n",
        "    print(f\"Best Model F1 Score: {best_val_f1:.5f}\")\n",
        "    print(f\"Model saved to {BEST_MODEL_SAVE_PATH}\")"
      ]
    }
  ]
}