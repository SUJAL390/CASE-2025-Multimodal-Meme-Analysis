{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApRYPwZt8RfG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import sys\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
        "D_CLIP_HIDDEN = 1024\n",
        "RESIDUAL_ALPHA = 0.2\n",
        "COSINE_CLASSIFIER_SCALE = 20.0\n",
        "OUTPUT_SIZE_D = 2\n",
        "LABEL_TO_ID_D = {'No Humor': 0, 'Humor': 1}\n",
        "ID_TO_LABEL_D = {v: k for k, v in LABEL_TO_ID_D.items()}\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "HEAD_ONLY_EPOCHS = 5\n",
        "FULL_MODEL_EPOCHS = 20\n",
        "LAYERS_TO_UNFREEZE = 2\n",
        "\n",
        "\n",
        "MAX_LR_HEAD = 1e-4\n",
        "MAX_LR_BACKBONE = 1e-6\n",
        "WEIGHT_DECAY = 1e-2\n",
        "LABEL_SMOOTHING = 0.1\n",
        "GRADIENT_CLIP_VAL = 1.0\n",
        "EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Random seed set to {seed}\")\n",
        "\n",
        "\n",
        "def load_data_D(split, text_csv, img_dir, label_csv=None, label_map=None):\n",
        "    \"\"\"Load and prepare dataset for training or validation.\"\"\"\n",
        "    text_df = pd.read_csv(text_csv).rename(columns={'index': 'name', 'text': 'text'})\n",
        "\n",
        "    if split == 'train':\n",
        "        data = []\n",
        "        for category, label_id in label_map.items():\n",
        "            folder = os.path.join(img_dir, category)\n",
        "            for img_path in glob.glob(os.path.join(folder, '*.*')):\n",
        "                name = os.path.basename(img_path)\n",
        "                text = text_df[text_df['name'] == name]['text'].values[0] if name in text_df['name'].values else \"\"\n",
        "                data.append({'name': name, 'text': text, 'label': label_id, 'img_path': img_path})\n",
        "        return pd.DataFrame(data)\n",
        "    else:  # validation set\n",
        "        labels_df = pd.read_csv(label_csv).rename(columns={'index': 'name', 'label': 'label'})\n",
        "        df = pd.merge(labels_df, text_df, on='name')\n",
        "        df['img_path'] = df['name'].apply(lambda x: os.path.join(img_dir, x))\n",
        "        return df\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for multimodal data.\"\"\"\n",
        "    def __init__(self, data_df, processor, transform):\n",
        "        self.data = data_df\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        label = torch.tensor(item['label'], dtype=torch.long)\n",
        "        text = self.processor.tokenizer(\n",
        "            str(item['text']), return_tensors=\"pt\", padding='max_length', truncation=True\n",
        "        ).input_ids.squeeze(0)\n",
        "\n",
        "        try:\n",
        "            image = self.transform(Image.open(item['img_path']).convert(\"RGB\"))\n",
        "        except Exception:\n",
        "            image = torch.zeros((3, 224, 224), dtype=torch.float)\n",
        "\n",
        "        return {\"image\": image, \"text\": text, \"label\": label}\n",
        "\n",
        "\n",
        "class FeatureAdapter(nn.Module):\n",
        "    \"\"\"A lightweight adapter module with residual connection.\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.adapter = nn.Sequential(nn.Linear(dim, dim // 2), nn.GELU(), nn.Linear(dim // 2, dim))\n",
        "    def forward(self, x, alpha):\n",
        "        return alpha * self.adapter(x) + (1.0 - alpha) * x\n",
        "\n",
        "class CosineClassifier(nn.Module):\n",
        "    \"\"\"Cosine similarity based classifier using learned prompt embeddings.\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, scale, prompts, clip_model, processor):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_dim, in_dim))\n",
        "        self.scale = scale\n",
        "        with torch.no_grad():\n",
        "            sai_proj = nn.Linear(clip_model.config.projection_dim, in_dim).to(clip_model.device)\n",
        "            tokenized = processor.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(clip_model.device)\n",
        "            raw_embeds = clip_model.get_text_features(**tokenized)\n",
        "            self.weight.data = F.normalize(sai_proj(raw_embeds.float()).data, p=2, dim=1)\n",
        "    def forward(self, x):\n",
        "        return F.linear(F.normalize(x, p=2, dim=1), F.normalize(self.weight, p=2, dim=1)) * self.scale\n",
        "\n",
        "class SotaMemeCLIP(nn.Module):\n",
        "    \"\"\"The full model combining CLIP with adapters and cosine classifier.\"\"\"\n",
        "    def __init__(self, clip_model, processor, num_classes, id_to_label_map):\n",
        "        super().__init__()\n",
        "        self.clip_model = clip_model\n",
        "        clip_dim = self.clip_model.config.projection_dim\n",
        "\n",
        "        self.image_proj = nn.Linear(clip_dim, D_CLIP_HIDDEN)\n",
        "        self.text_proj = nn.Linear(clip_dim, D_CLIP_HIDDEN)\n",
        "        self.image_adapter = FeatureAdapter(D_CLIP_HIDDEN)\n",
        "        self.text_adapter = FeatureAdapter(D_CLIP_HIDDEN)\n",
        "        self.pre_output = nn.Sequential(nn.Dropout(0.1), nn.Linear(D_CLIP_HIDDEN, D_CLIP_HIDDEN), nn.GELU(), nn.Dropout(0.1))\n",
        "\n",
        "        prompts = [f\"This is a meme about {id_to_label_map[i]}\" for i in range(num_classes)]\n",
        "        self.cosine_classifier = CosineClassifier(D_CLIP_HIDDEN, num_classes, COSINE_CLASSIFIER_SCALE, prompts, clip_model, processor)\n",
        "\n",
        "    def forward(self, images, texts):\n",
        "        with torch.set_grad_enabled(self.training):\n",
        "            img_features = self.clip_model.get_image_features(pixel_values=images)\n",
        "            text_features = self.clip_model.get_text_features(input_ids=texts)\n",
        "\n",
        "        img_proj = self.image_proj(img_features)\n",
        "        text_proj = self.text_proj(text_features)\n",
        "        img_adapted = self.image_adapter(img_proj, RESIDUAL_ALPHA)\n",
        "        text_adapted = self.text_adapter(text_proj, RESIDUAL_ALPHA)\n",
        "\n",
        "        fused = img_adapted * text_adapted\n",
        "        pre_out = self.pre_output(fused)\n",
        "        logits = self.cosine_classifier(pre_out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def compute_loss(logits, targets, smoothing=0.0):\n",
        "    \"\"\"Compute cross-entropy loss with optional label smoothing.\"\"\"\n",
        "    if smoothing > 0:\n",
        "        n_classes = logits.size(1)\n",
        "        smooth_labels = (1 - smoothing) * F.one_hot(targets, num_classes=n_classes) + smoothing / n_classes\n",
        "        log_probs = F.log_softmax(logits, dim=1)\n",
        "        loss = -(smooth_labels * log_probs).sum(dim=1).mean()\n",
        "    else:\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "    return loss\n",
        "\n",
        "def calculate_metrics(preds, labels):\n",
        "    \"\"\"Compute weighted F1-score.\"\"\"\n",
        "    return f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, path)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, model_save_path, optimizer, scheduler=None):\n",
        "    \"\"\"Main two-stage training loop.\"\"\"\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for stage, epochs in enumerate([HEAD_ONLY_EPOCHS, FULL_MODEL_EPOCHS], start=1):\n",
        "        print(f\"\\n=== Training Stage {stage} for {epochs} epochs ===\")\n",
        "\n",
        "        if stage == 1:\n",
        "            for param in model.clip_model.parameters(): param.requires_grad = False\n",
        "            for param in model.parameters():\n",
        "                if any(p is param for p in model.clip_model.parameters()): continue\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            for param in model.clip_model.parameters(): param.requires_grad = False\n",
        "            for i in range(-LAYERS_TO_UNFREEZE, 0):\n",
        "                for param in model.clip_model.vision_model.encoder.layers[i].parameters(): param.requires_grad = True\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False):\n",
        "                images, texts, labels = batch['image'].to(DEVICE), batch['text'].to(DEVICE), batch['label'].to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images, texts)\n",
        "                loss = compute_loss(outputs, labels, LABEL_SMOOTHING)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_VAL)\n",
        "                optimizer.step()\n",
        "                if scheduler: scheduler.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "            model.eval()\n",
        "            val_preds, val_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    images, texts, labels = batch['image'].to(DEVICE), batch['text'].to(DEVICE), batch['label'].to(DEVICE)\n",
        "                    outputs = model(images, texts)\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                    val_preds.append(preds); val_labels.append(labels)\n",
        "\n",
        "            val_preds = torch.cat(val_preds); val_labels = torch.cat(val_labels)\n",
        "            val_f1 = calculate_metrics(val_preds, val_labels)\n",
        "            print(f\"Epoch {epoch + 1}: Train Loss={avg_train_loss:.4f} | Val Weighted F1={val_f1:.4f}\")\n",
        "\n",
        "            if val_f1 > best_val_f1:\n",
        "                best_val_f1 = val_f1; patience_counter = 0\n",
        "                checkpoint_path = os.path.join(model_save_path, f\"best_model_stage{stage}.pt\")\n",
        "                save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
        "                print(f\"Saved best model at epoch {epoch + 1} with F1: {val_f1:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                    print(f\"No improvement for {EARLY_STOPPING_PATIENCE} epochs, early stopping...\")\n",
        "                    return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train MemeCLIP model for Subtask D: Humor Detection.\")\n",
        "    parser.add_argument(\n",
        "        '--base_dir',\n",
        "        type=str,\n",
        "        default=\"./SharedTaskProject\",\n",
        "        help=\"Path to the base project directory containing the Subtask data folders.\"\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    seed_everything(SEED)\n",
        "\n",
        "\n",
        "    BASE_PROJECT_DIR = args.base_dir\n",
        "    MODELS_DIR_D = os.path.join(BASE_PROJECT_DIR, \"models\", \"Subtask_D\")\n",
        "    EXP_NAME_D = \"memeclip_humor_sota_v1\"\n",
        "    MODEL_SAVE_PATH_D = os.path.join(MODELS_DIR_D, EXP_NAME_D)\n",
        "    Path(MODEL_SAVE_PATH_D).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"This run's models will be saved in: {MODEL_SAVE_PATH_D}\")\n",
        "\n",
        "    TRAIN_IMAGES_ROOT_DIR_D = os.path.join(BASE_PROJECT_DIR, \"Subtask_D\", \"Train\", \"Subtask_D_Train\")\n",
        "    TRAIN_TEXT_CSV_FILE_D = os.path.join(BASE_PROJECT_DIR, \"Train_Text\", \"STask_D_train.csv\")\n",
        "    VAL_IMAGES_DIR_D = os.path.join(BASE_PROJECT_DIR, \"Subtask_D\", \"Evaluation\", \"STask_D_val_img\")\n",
        "    VAL_TEXT_CSV_FILE_D = os.path.join(BASE_PROJECT_DIR, \"Eval_Data_Text\", \"STask-D(index,text)val.csv\")\n",
        "    VAL_LABELS_CSV_FILE_D = os.path.join(BASE_PROJECT_DIR, \"Eval_Data_Labels\", \"STask-D(index,label)val.csv\")\n",
        "\n",
        "\n",
        "    print(f\"Loading CLIP model and processor: {CLIP_MODEL_ID}...\")\n",
        "    try:\n",
        "        processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)\n",
        "        clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE)\n",
        "    except Exception as e:\n",
        "        sys.exit(f\"ERROR: Could not load CLIP model: {e}.\")\n",
        "\n",
        "    print(\"Loading and preparing datasets...\")\n",
        "    train_df = load_data_D('train', TRAIN_TEXT_CSV_FILE_D, TRAIN_IMAGES_ROOT_DIR_D, label_map=LABEL_TO_ID_D)\n",
        "    val_df = load_data_D('val', VAL_TEXT_CSV_FILE_D, VAL_IMAGES_DIR_D, VAL_LABELS_CSV_FILE_D)\n",
        "\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = MultimodalDataset(train_df, processor, train_transform)\n",
        "    val_dataset = MultimodalDataset(val_df, processor, val_transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "\n",
        "    model = SotaMemeCLIP(clip_model, processor, OUTPUT_SIZE_D, ID_TO_LABEL_D).to(DEVICE)\n",
        "    params = [\n",
        "        {'params': model.clip_model.parameters(), 'lr': MAX_LR_BACKBONE},\n",
        "        {'params': model.image_proj.parameters(), 'lr': MAX_LR_HEAD},\n",
        "        {'params': model.text_proj.parameters(), 'lr': MAX_LR_HEAD},\n",
        "        {'params': model.image_adapter.parameters(), 'lr': MAX_LR_HEAD},\n",
        "        {'params': model.text_adapter.parameters(), 'lr': MAX_LR_HEAD},\n",
        "        {'params': model.pre_output.parameters(), 'lr': MAX_LR_HEAD},\n",
        "        {'params': model.cosine_classifier.parameters(), 'lr': MAX_LR_HEAD},\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = None\n",
        "\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_loader, val_loader, MODEL_SAVE_PATH_D, optimizer, scheduler)\n",
        "    print(\"Training completed!\")"
      ]
    }
  ]
}