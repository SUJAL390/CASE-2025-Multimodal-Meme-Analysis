{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: SETUP AND DRIVE MOUNT\n",
        "# ==============================================================================\n",
        "# Run this cell once at the very beginning of your session.\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"--- Step 0: Mounting Google Drive and Initial Setup ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    sys.exit(f\"CRITICAL ERROR: Could not mount Google Drive. The script cannot continue. Error: {e}\")\n",
        "\n",
        "# Define global constants that all parts will use\n",
        "BASE_PROJECT_DIR = \"/content/drive/.shortcut-targets-by-id/1cLgae9ycX3zn2wP-kQw-fVulnU8UEAUV/SharedTaskProject/\"\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# This ensures subsequent cells can find the BASE_PROJECT_DIR and DEVICE variables\n",
        "print(\"\\nSetup complete. You can now proceed to Cell 2.\")"
      ],
      "metadata": {
        "id": "8N5Ef-9RFoIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eadd0a3a-6ab3-446f-c9a0-b015ddc23fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 0: Mounting Google Drive and Initial Setup ---\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Using device: cuda\n",
            "\n",
            "Setup complete. You can now proceed to Cell 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_E9OBx4Fhvw"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: PART 1 - PRE-COMPUTE FEATURES\n",
        "# ==============================================================================\n",
        "# Run this cell ONCE per dataset. This is the slow part.\n",
        "# After this completes, the features are saved to your Drive forever.\n",
        "# You do not need to run this cell again in future sessions.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import warnings\n",
        "\n",
        "# --- Configuration for Pre-computation ---\n",
        "class PrecomputeConfig:\n",
        "    FEATURES_DIR = os.path.join(BASE_PROJECT_DIR, \"precomputed_features\", \"Subtask_A_Final\")\n",
        "    CLIP_MODEL_ID = \"openai/clip-vit-large-patch14\"\n",
        "    BATCH_SIZE = 64\n",
        "    LABEL_COL, IMAGE_NAME_COL, TEXT_COL = 'label', 'index', 'text'\n",
        "    LABEL_TO_ID = {'No Hate': 0, 'Hate': 1}\n",
        "    TRAIN_IMAGES_ROOT_DIR = os.path.join(BASE_PROJECT_DIR, \"Subtask_A\", \"Train\", \"Subtask_A_Train\")\n",
        "    TRAIN_TEXT_CSV_FILE = os.path.join(BASE_PROJECT_DIR, \"Train_Text\", \"STask_A_train.csv\")\n",
        "    VAL_IMAGES_DIR = os.path.join(BASE_PROJECT_DIR, \"Subtask_A\", \"Evaluation\", \"STask_A_val_img\")\n",
        "    VAL_TEXT_CSV_FILE = os.path.join(BASE_PROJECT_DIR, \"Eval_Data_Text\", \"STask-A(index,text)val.csv\")\n",
        "    VAL_LABELS_CSV_FILE = os.path.join(BASE_PROJECT_DIR, \"Eval_Data_Labels\", \"STask-A(index,label)val.csv\")\n",
        "    TEST_IMAGES_DIR = os.path.join(BASE_PROJECT_DIR, \"Subtask_A\", \"Test\", \"STask_A_test_img\")\n",
        "    TEST_TEXT_CSV_FILE = os.path.join(BASE_PROJECT_DIR, \"Subtask_A\", \"Test\", \"STask-A(index,text)test.csv\")\n",
        "\n",
        "cfg_precompute = PrecomputeConfig()\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "os.makedirs(cfg_precompute.FEATURES_DIR, exist_ok=True)\n",
        "print(f\"--- Part 1: Pre-computation Script ---\")\n",
        "print(f\"Saving features to: {cfg_precompute.FEATURES_DIR}\")\n",
        "\n",
        "def load_data_for_precompute(split):\n",
        "    if split == 'train':\n",
        "        img_root_dir, text_csv_path = cfg_precompute.TRAIN_IMAGES_ROOT_DIR, cfg_precompute.TRAIN_TEXT_CSV_FILE\n",
        "        text_df = pd.read_csv(text_csv_path).rename(columns={cfg_precompute.IMAGE_NAME_COL: 'name', cfg_precompute.TEXT_COL: 'text'})\n",
        "        data_list = []\n",
        "        for category, label_id in cfg_precompute.LABEL_TO_ID.items():\n",
        "            folder_path = os.path.join(img_root_dir, category)\n",
        "            if not os.path.isdir(folder_path): continue\n",
        "            for ext in ('*.png', '*.jpg', '*.jpeg'):\n",
        "                for img_path in glob.glob(os.path.join(folder_path, ext)):\n",
        "                    img_filename = os.path.basename(img_path)\n",
        "                    text_row = text_df[text_df['name'] == img_filename]\n",
        "                    text = str(text_row.iloc[0]['text']) if not text_row.empty and pd.notna(text_row.iloc[0]['text']) else \"\"\n",
        "                    data_list.append({'name': img_filename, 'text': text, cfg_precompute.LABEL_COL: label_id, 'img_path': img_path})\n",
        "        return pd.DataFrame(data_list)\n",
        "    elif split == 'val':\n",
        "        labels_df = pd.read_csv(cfg_precompute.VAL_LABELS_CSV_FILE).rename(columns={cfg_precompute.IMAGE_NAME_COL: 'name', 'label': cfg_precompute.LABEL_COL})\n",
        "        text_df = pd.read_csv(cfg_precompute.VAL_TEXT_CSV_FILE).rename(columns={cfg_precompute.IMAGE_NAME_COL: 'name', cfg_precompute.TEXT_COL: 'text'})\n",
        "        df = pd.merge(labels_df, text_df, on='name', how='inner')\n",
        "        df[cfg_precompute.LABEL_COL] = df[cfg_precompute.LABEL_COL].astype(int)\n",
        "        df['img_path'] = df['name'].apply(lambda x: os.path.join(cfg_precompute.VAL_IMAGES_DIR, x))\n",
        "        return df\n",
        "    elif split == 'test':\n",
        "        df = pd.read_csv(cfg_precompute.TEST_TEXT_CSV_FILE).rename(columns={'index': 'name', 'text': 'text'})\n",
        "        df['img_path'] = df['name'].apply(lambda x: os.path.join(cfg_precompute.TEST_IMAGES_DIR, x))\n",
        "        # Add a dummy label column for dataloader compatibility\n",
        "        df[cfg_precompute.LABEL_COL] = -1\n",
        "        return df\n",
        "\n",
        "print(f\"Loading CLIP model: {cfg_precompute.CLIP_MODEL_ID}\")\n",
        "processor = CLIPProcessor.from_pretrained(cfg_precompute.CLIP_MODEL_ID)\n",
        "model_clip = CLIPModel.from_pretrained(cfg_precompute.CLIP_MODEL_ID).to(DEVICE).eval()\n",
        "\n",
        "splits_to_process = ['test']\n",
        "\n",
        "for split in splits_to_process:\n",
        "    print(f\"\\n--- Processing {split} split ---\")\n",
        "    df = load_data_for_precompute(split)\n",
        "\n",
        "    if df.empty:\n",
        "        print(f\"Warning: No data found for split '{split}'. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Sort test set now to preserve order for final submission\n",
        "    if split == 'test':\n",
        "        # Handle cases where the test set might not have numeric filenames\n",
        "        try:\n",
        "            df['sort_key'] = df['name'].str.extract('(\\d+)').astype(int)\n",
        "            df = df.sort_values(by='sort_key').drop(columns=['sort_key']).reset_index(drop=True)\n",
        "        except:\n",
        "            print(\"Could not sort test files numerically, using alphabetical sort.\")\n",
        "            df = df.sort_values(by='name').reset_index(drop=True)\n",
        "\n",
        "        # Save the sorted names for later\n",
        "        df[['name']].to_csv(os.path.join(cfg_precompute.FEATURES_DIR, \"test_names_sorted.csv\"), index=False)\n",
        "\n",
        "    all_img_features, all_txt_features, all_labels = [], [], []\n",
        "    class InferenceDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, df): self.df = df\n",
        "        def __len__(self): return len(self.df)\n",
        "        def __getitem__(self, idx): return self.df.iloc[idx].to_dict()\n",
        "    dataloader = torch.utils.data.DataLoader(InferenceDataset(df), batch_size=cfg_precompute.BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=f\"Extracting {split} features\"):\n",
        "            # This try-except block makes image loading more robust.\n",
        "            try:\n",
        "                images = [Image.open(p).convert(\"RGB\") for p in batch['img_path']]\n",
        "                image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "                img_features = model_clip.get_image_features(**image_inputs)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: A batch of images could not be loaded. Using zero features. Error: {e}\")\n",
        "                img_features = torch.zeros((len(batch['img_path']), model_clip.projection_dim)).to(DEVICE)\n",
        "\n",
        "            texts = list(batch['text'])\n",
        "            text_inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
        "            txt_features = model_clip.get_text_features(**text_inputs)\n",
        "            all_img_features.append(img_features.cpu()); all_txt_features.append(txt_features.cpu())\n",
        "            if split != 'test':\n",
        "                all_labels.append(torch.tensor(batch['label'].tolist(), dtype=torch.long))\n",
        "\n",
        "    img_features_tensor = torch.cat(all_img_features, dim=0)\n",
        "    txt_features_tensor = torch.cat(all_txt_features, dim=0)\n",
        "\n",
        "    torch.save(img_features_tensor, os.path.join(cfg_precompute.FEATURES_DIR, f\"{split}_img_features.pt\"))\n",
        "    torch.save(txt_features_tensor, os.path.join(cfg_precompute.FEATURES_DIR, f\"{split}_txt_features.pt\"))\n",
        "\n",
        "    if split != 'test':\n",
        "        labels_tensor = torch.cat(all_labels, dim=0)\n",
        "        torch.save(labels_tensor, os.path.join(cfg_precompute.FEATURES_DIR, f\"{split}_labels.pt\"))\n",
        "\n",
        "    print(f\"Successfully saved all features for {split} split.\")\n",
        "\n",
        "del model_clip, processor\n",
        "torch.cuda.empty_cache()\n",
        "print(\"\\n--- PART 1 COMPLETE. YOU CAN NOW RUN CELL 3. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: PART 2 & 3 (FINAL \"RADICAL SIMPLICITY\" VERSION)\n",
        "# ==============================================================================\n",
        "# This script uses the most robust and reliable training setup: AdamW with a\n",
        "# fixed learning rate. All experimental complexity has been removed.\n",
        "# This is the definitive strategy to achieve a high score.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import warnings\n",
        "import json\n",
        "import zipfile\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "# ==============================================================================\n",
        "# CELL 1: SETUP AND DRIVE MOUNT\n",
        "# ==============================================================================\n",
        "# Run this cell once at the very beginning of your session.\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import torch\n",
        "import os\n",
        "\n",
        "print(\"--- Step 0: Mounting Google Drive and Initial Setup ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    sys.exit(f\"CRITICAL ERROR: Could not mount Google Drive. The script cannot continue. Error: {e}\")\n",
        "\n",
        "# Define global constants that all parts will use\n",
        "BASE_PROJECT_DIR = \"/content/drive/.shortcut-targets-by-id/1cLgae9ycX3zn2wP-kQw-fVulnU8UEAUV/SharedTaskProject/\"\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# This ensures subsequent cells can find the BASE_PROJECT_DIR and DEVICE variables\n",
        "print(\"\\nSetup complete. You can now proceed to Cell 2.\")\n",
        "\n",
        "# --- 1. SETUP & ADVANCED CONFIGURATION ---\n",
        "print(\"--- Final Training & Submission Script (Radical Simplicity Version) ---\")\n",
        "print(f\"DEVICE: {DEVICE.upper()}\")\n",
        "\n",
        "# --- CORE PATHS CONFIGURATION ---\n",
        "class FinalConfig:\n",
        "    FEATURES_DIR = Path(BASE_PROJECT_DIR) / \"precomputed_features\" / \"Subtask_A_Final\"\n",
        "    MODELS_DIR = Path(BASE_PROJECT_DIR) / \"models\" / \"Subtask_A\"\n",
        "    EXP_NAME = \"memeclip_COATTENTION_RADICAL_SIMPLICITY\"\n",
        "    MODEL_SAVE_PATH = MODELS_DIR / EXP_NAME\n",
        "    SUBMISSION_DIR = Path(BASE_PROJECT_DIR) / \"submissions\" / \"Subtask_A_Radical_Simplicity\"\n",
        "    SUBMISSION_JSON_PATH = SUBMISSION_DIR / \"submission.json\"\n",
        "    SUBMISSION_ZIP_PATH = SUBMISSION_DIR / \"ref.zip\"\n",
        "\n",
        "    # --- Ensemble & Training Strategy ---\n",
        "    NUM_ENSEMBLE_MODELS = 5\n",
        "    NUM_EPOCHS = 40 # Give it plenty of time to converge with a small LR\n",
        "    BATCH_SIZE = 1024\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "\n",
        "    # --- Optimizer (NO SCHEDULER) ---\n",
        "    LEARNING_RATE = 2e-4 # A small, fixed learning rate\n",
        "    WEIGHT_DECAY = 0.1\n",
        "\n",
        "    # --- SOTA Co-Attention Model Architecture ---\n",
        "    FEATURE_DIM = 768\n",
        "    ATTENTION_HEADS = 8\n",
        "    ATTENTION_LAYERS = 4\n",
        "    ATTENTION_DROPOUT = 0.25\n",
        "    OUTPUT_DIM = 2\n",
        "\n",
        "    INFERENCE_BATCH_SIZE = 2048\n",
        "    SEEDS = [2277, 1365, 2614, 727, 206924]\n",
        "\n",
        "cfg = FinalConfig()\n",
        "cfg.MODEL_SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "cfg.SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 2. RE-DEFINE MODEL ARCHITECTURE & TRAINING LOGIC ---\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class CoAttentionClassifier(nn.Module):\n",
        "    def __init__(self, feature_dim, num_heads, num_layers, dropout, output_dim):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.img_proj = nn.Linear(feature_dim, feature_dim)\n",
        "        self.txt_proj = nn.Linear(feature_dim, feature_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads, dropout=dropout, batch_first=True, activation='gelu', dim_feedforward=feature_dim*4)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier_head = nn.Sequential(\n",
        "            nn.LayerNorm(feature_dim * 2),\n",
        "            nn.Linear(feature_dim * 2, feature_dim), nn.GELU(),\n",
        "            nn.Dropout(dropout), nn.Linear(feature_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, img_feat, txt_feat):\n",
        "        img_proj = self.img_proj(img_feat).unsqueeze(1)\n",
        "        txt_proj = self.txt_proj(txt_feat).unsqueeze(1)\n",
        "        combined_seq = torch.cat([img_proj, txt_proj], dim=1)\n",
        "        attended_seq = self.transformer_encoder(combined_seq)\n",
        "        flat_features = attended_seq.flatten(start_dim=1)\n",
        "        return self.classifier_head(flat_features)\n",
        "\n",
        "def train_one_model(seed, train_loader, val_loader, val_labels_np):\n",
        "    seed_everything(seed)\n",
        "    print(f\"\\n--- Training Model with seed {seed} ---\")\n",
        "    model = CoAttentionClassifier(cfg.FEATURE_DIM, cfg.ATTENTION_HEADS, cfg.ATTENTION_LAYERS, cfg.ATTENTION_DROPOUT, cfg.OUTPUT_DIM).to(DEVICE)\n",
        "\n",
        "    # The most robust setup: AdamW with a fixed learning rate.\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "    early_stopping_patience = 7 # Stop if no improvement for 7 epochs\n",
        "\n",
        "    for epoch in range(cfg.NUM_EPOCHS):\n",
        "        model.train()\n",
        "        for img_feat, txt_feat, labels in train_loader:\n",
        "            img_feat, txt_feat, labels = img_feat.to(DEVICE), txt_feat.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(img_feat, txt_feat)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds_probas = []\n",
        "        with torch.no_grad():\n",
        "            for img_feat, txt_feat, _ in val_loader:\n",
        "                img_feat, txt_feat = img_feat.to(DEVICE), txt_feat.to(DEVICE)\n",
        "                all_preds_probas.append(torch.softmax(model(img_feat, txt_feat), dim=1).cpu())\n",
        "        val_preds_labels = torch.argmax(torch.cat(all_preds_probas, dim=0), dim=1)\n",
        "        val_f1 = f1_score(val_labels_np, val_preds_labels.numpy(), average='weighted')\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            patience_counter = 0\n",
        "            print(f\"  Epoch {epoch+1}/{cfg.NUM_EPOCHS} -> New Best Val F1: {val_f1:.4f}\")\n",
        "            torch.save(model.state_dict(), cfg.MODEL_SAVE_PATH / f\"simple_adamw_seed_{seed}.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= early_stopping_patience:\n",
        "            print(f\"  Early stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    print(f\"  Finished training for seed {seed}. Best Val F1: {best_val_f1:.4f}\")\n",
        "\n",
        "# --- 3. EXECUTE TRAINING & SUBMISSION PIPELINE ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"--- PART 2: TRAINING SOTA CO-ATTENTION ENSEMBLE (ROBUST) ---\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Loading pre-computed features...\")\n",
        "try:\n",
        "    train_img_feat = torch.load(cfg.FEATURES_DIR / \"train_img_features.pt\")\n",
        "    train_txt_feat = torch.load(cfg.FEATURES_DIR / \"train_txt_features.pt\")\n",
        "    train_labels = torch.load(cfg.FEATURES_DIR / \"train_labels.pt\")\n",
        "    val_img_feat = torch.load(cfg.FEATURES_DIR / \"val_img_features.pt\")\n",
        "    val_txt_feat = torch.load(cfg.FEATURES_DIR / \"val_txt_features.pt\")\n",
        "    val_labels = torch.load(cfg.FEATURES_DIR / \"val_labels.pt\")\n",
        "except FileNotFoundError as e:\n",
        "    sys.exit(f\"FATAL ERROR: Pre-computed feature files not found. Please run Cell 2 first. Missing file: {e}\")\n",
        "\n",
        "val_labels_np = val_labels.numpy()\n",
        "train_loader = DataLoader(TensorDataset(train_img_feat, train_txt_feat, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(TensorDataset(val_img_feat, val_txt_feat, val_labels), batch_size=cfg.BATCH_SIZE * 2, num_workers=2)\n",
        "\n",
        "for seed in cfg.SEEDS:\n",
        "    train_one_model(seed, train_loader, val_loader, val_labels_np)\n",
        "\n",
        "print(\"\\n--- ENSEMBLE TRAINING COMPLETE ---\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"--- PART 3: GENERATING FINAL SUBMISSION ---\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Loading pre-computed test features...\")\n",
        "test_img_feat = torch.load(cfg.FEATURES_DIR / \"test_img_features.pt\")\n",
        "test_txt_feat = torch.load(cfg.FEATURES_DIR / \"test_txt_features.pt\")\n",
        "test_names_df = pd.read_csv(cfg.FEATURES_DIR / \"test_names_sorted.csv\")\n",
        "\n",
        "print(\"Loading trained ensemble models...\")\n",
        "models = []\n",
        "for seed in cfg.SEEDS:\n",
        "    model_path = cfg.MODEL_SAVE_PATH / f\"simple_adamw_seed_{seed}.pt\"\n",
        "    model = CoAttentionClassifier(cfg.FEATURE_DIM, cfg.ATTENTION_HEADS, cfg.ATTENTION_LAYERS, cfg.ATTENTION_DROPOUT, cfg.OUTPUT_DIM).to(DEVICE)\n",
        "    # Use a try-except block here in case one of the models failed to save (e.g., if it never improved)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "        model.eval()\n",
        "        models.append(model)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Model for seed {seed} not found at {model_path}. It might not have improved past epoch 0. Skipping.\")\n",
        "\n",
        "print(f\"Successfully loaded {len(models)} models for ensembling.\")\n",
        "\n",
        "if not models:\n",
        "    sys.exit(\"FATAL ERROR: No models were successfully loaded. Cannot create submission.\")\n",
        "\n",
        "inference_loader = DataLoader(TensorDataset(test_img_feat, test_txt_feat), batch_size=cfg.INFERENCE_BATCH_SIZE, shuffle=False)\n",
        "all_model_probas = []\n",
        "with torch.no_grad():\n",
        "    for model in tqdm(models, desc=\"Ensemble Inference on Test Set\"):\n",
        "        current_model_probas = []\n",
        "        for img_feat, txt_feat in inference_loader:\n",
        "            img_feat, txt_feat = img_feat.to(DEVICE), txt_feat.to(DEVICE)\n",
        "            logits = model(img_feat, txt_feat)\n",
        "            current_model_probas.append(torch.softmax(logits, dim=1).cpu())\n",
        "        all_model_probas.append(torch.cat(current_model_probas, dim=0))\n",
        "\n",
        "ensemble_probas = torch.stack(all_model_probas, dim=0).mean(dim=0)\n",
        "final_predictions = torch.argmax(ensemble_probas, dim=1).numpy()\n",
        "\n",
        "print(\"\\nCreating submission file in the specified format...\")\n",
        "submission_df = pd.DataFrame({'index': test_names_df['name'], 'prediction': final_predictions})\n",
        "submission_list = submission_df.to_dict('records')\n",
        "\n",
        "with open(cfg.SUBMISSION_JSON_PATH, 'w') as f:\n",
        "    for item in submission_list:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "print(f\"submission.json created successfully at: {cfg.SUBMISSION_JSON_PATH}\")\n",
        "\n",
        "print(\"\\nZipping submission file for CodaLab...\")\n",
        "with zipfile.ZipFile(cfg.SUBMISSION_ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    zf.write(cfg.SUBMISSION_JSON_PATH, arcname='submission.json')\n",
        "print(f\"\\n✅ Submission ready! ref.zip created at: {cfg.SUBMISSION_ZIP_PATH}\")\n",
        "\n",
        "print(\"\\n\" + \"*\"*80)\n",
        "print(\"--- MISSION ACCOMPLISHED. THE DEFINITIVE SUBMISSION IS READY. ---\")\n",
        "print(\"*\"*80)"
      ],
      "metadata": {
        "id": "pwX2G7yadkGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54cb39d-8d5f-4633-9ce2-bae9aa93a38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 0: Mounting Google Drive and Initial Setup ---\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Using device: cuda\n",
            "\n",
            "Setup complete. You can now proceed to Cell 2.\n",
            "--- Final Training & Submission Script (Radical Simplicity Version) ---\n",
            "DEVICE: CUDA\n",
            "\n",
            "================================================================================\n",
            "--- PART 2: TRAINING SOTA CO-ATTENTION ENSEMBLE (ROBUST) ---\n",
            "================================================================================\n",
            "Loading pre-computed features...\n",
            "\n",
            "--- Training Model with seed 2277 ---\n",
            "  Epoch 1/40 -> New Best Val F1: 0.3479\n",
            "  Epoch 2/40 -> New Best Val F1: 0.7586\n",
            "  Epoch 3/40 -> New Best Val F1: 0.7729\n",
            "  Epoch 4/40 -> New Best Val F1: 0.7904\n",
            "  Epoch 5/40 -> New Best Val F1: 0.7981\n",
            "  Epoch 6/40 -> New Best Val F1: 0.8098\n",
            "  Epoch 7/40 -> New Best Val F1: 0.8198\n",
            "  Early stopping triggered at epoch 14.\n",
            "  Finished training for seed 2277. Best Val F1: 0.8198\n",
            "\n",
            "--- Training Model with seed 1365 ---\n",
            "  Epoch 1/40 -> New Best Val F1: 0.4502\n",
            "  Epoch 2/40 -> New Best Val F1: 0.7613\n",
            "  Epoch 3/40 -> New Best Val F1: 0.7812\n",
            "  Epoch 4/40 -> New Best Val F1: 0.7967\n",
            "  Epoch 5/40 -> New Best Val F1: 0.7992\n",
            "  Epoch 6/40 -> New Best Val F1: 0.8140\n",
            "  Epoch 7/40 -> New Best Val F1: 0.8259\n",
            "  Early stopping triggered at epoch 14.\n",
            "  Finished training for seed 1365. Best Val F1: 0.8259\n",
            "\n",
            "--- Training Model with seed 2614 ---\n",
            "  Epoch 1/40 -> New Best Val F1: 0.6290\n",
            "  Epoch 2/40 -> New Best Val F1: 0.6776\n",
            "  Epoch 3/40 -> New Best Val F1: 0.7684\n",
            "  Epoch 4/40 -> New Best Val F1: 0.7796\n",
            "  Epoch 5/40 -> New Best Val F1: 0.8043\n",
            "  Epoch 7/40 -> New Best Val F1: 0.8181\n",
            "  Early stopping triggered at epoch 14.\n",
            "  Finished training for seed 2614. Best Val F1: 0.8181\n",
            "\n",
            "--- Training Model with seed 727 ---\n",
            "  Epoch 1/40 -> New Best Val F1: 0.3859\n",
            "  Epoch 2/40 -> New Best Val F1: 0.7487\n",
            "  Epoch 3/40 -> New Best Val F1: 0.7653\n",
            "  Epoch 4/40 -> New Best Val F1: 0.7799\n",
            "  Epoch 5/40 -> New Best Val F1: 0.7941\n",
            "  Epoch 6/40 -> New Best Val F1: 0.8043\n",
            "  Epoch 7/40 -> New Best Val F1: 0.8120\n",
            "  Early stopping triggered at epoch 14.\n",
            "  Finished training for seed 727. Best Val F1: 0.8120\n",
            "\n",
            "--- Training Model with seed 206924 ---\n",
            "  Epoch 1/40 -> New Best Val F1: 0.3444\n",
            "  Epoch 2/40 -> New Best Val F1: 0.7767\n",
            "  Epoch 3/40 -> New Best Val F1: 0.7927\n",
            "  Epoch 4/40 -> New Best Val F1: 0.7935\n",
            "  Epoch 5/40 -> New Best Val F1: 0.8016\n",
            "  Epoch 7/40 -> New Best Val F1: 0.8140\n",
            "  Epoch 8/40 -> New Best Val F1: 0.8201\n",
            "  Early stopping triggered at epoch 15.\n",
            "  Finished training for seed 206924. Best Val F1: 0.8201\n",
            "\n",
            "--- ENSEMBLE TRAINING COMPLETE ---\n",
            "\n",
            "================================================================================\n",
            "--- PART 3: GENERATING FINAL SUBMISSION ---\n",
            "================================================================================\n",
            "Loading pre-computed test features...\n",
            "Loading trained ensemble models...\n",
            "Successfully loaded 5 models for ensembling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ensemble Inference on Test Set: 100%|██████████| 5/5 [00:00<00:00, 23.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating submission file in the specified format...\n",
            "submission.json created successfully at: /content/drive/.shortcut-targets-by-id/1cLgae9ycX3zn2wP-kQw-fVulnU8UEAUV/SharedTaskProject/submissions/Subtask_A_Radical_Simplicity/submission.json\n",
            "\n",
            "Zipping submission file for CodaLab...\n",
            "\n",
            "✅ Submission ready! ref.zip created at: /content/drive/.shortcut-targets-by-id/1cLgae9ycX3zn2wP-kQw-fVulnU8UEAUV/SharedTaskProject/submissions/Subtask_A_Radical_Simplicity/ref.zip\n",
            "\n",
            "********************************************************************************\n",
            "--- MISSION ACCOMPLISHED. THE DEFINITIVE SUBMISSION IS READY. ---\n",
            "********************************************************************************\n"
          ]
        }
      ]
    }
  ]
}